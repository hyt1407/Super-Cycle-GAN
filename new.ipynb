{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22110,
     "status": "ok",
     "timestamp": 1563795161421,
     "user": {
      "displayName": "黄翼",
      "photoUrl": "",
      "userId": "03920361838354357255"
     },
     "user_tz": -480
    },
    "id": "UBGZ2iFuJh2K",
    "outputId": "a471fe2b-bde2-4e21-fd4b-75a4bc8ad7ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1563795166110,
     "user": {
      "displayName": "黄翼",
      "photoUrl": "",
      "userId": "03920361838354357255"
     },
     "user_tz": -480
    },
    "id": "aiogRU_iJjZa",
    "outputId": "7ead16eb-4cdf-4ca9-aa03-40b4b744de05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/CycleGAN\n"
     ]
    }
   ],
   "source": [
    "#cd drive/My Drive/CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6999,
     "status": "ok",
     "timestamp": 1563795179943,
     "user": {
      "displayName": "黄翼",
      "photoUrl": "",
      "userId": "03920361838354357255"
     },
     "user_tz": -480
    },
    "id": "y_SPea5eJjcT",
    "outputId": "577caaf8-4cef-4157-a5dc-0aabf984ae36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.1.0 from https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.16.4)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.16.4)\n",
      "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.1.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n"
     ]
    }
   ],
   "source": [
    "#!pip install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n",
    "#!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IKobDYqHJjeY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dH7GbC93JV6I"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kada-W_OJV6L"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 8\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjVUDaYbJV6N"
   },
   "source": [
    "### 以下定义生成器与判别器的网络结构\n",
    "\n",
    "- 生成器和判别器均使用孪生网络  \n",
    "- 生成器使用RNN方式，第一个step生成local文件，第二个step生成全局图像。  \n",
    "- 生成器step1和step2使用同一个骨干网络\n",
    "- 判别器最后一层不要用sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XtmBJqYVJV6O"
   },
   "outputs": [],
   "source": [
    "#每个local的输出的分支网络（如果需要的话）（如果输入也要分支也可以用这个）\n",
    "class outBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels):\n",
    "        super(outBlock,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels,32,kernel_size=(3,3),stride=1,padding=1),\n",
    "        nn.BatchNorm2d(32),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential(\n",
    "        nn.Conv2d(32,64,kernel_size=1,stride=1,padding=0),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.block = nn.Sequential(\n",
    "        nn.Conv2d(32,64,kernel_size = 1,stride = 1,padding = 0),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(inplace = True),\n",
    "        nn.Conv2d(64,64,kernel_size = 3,stride = 1,padding = 1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.sqush = nn.Conv2d(128,out_channels,kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x1 = self.shortcut(x)\n",
    "        x2 = self.block(x)\n",
    "        x = torch.cat((x1,x2),1)\n",
    "        return self.sqush(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNPSYJKfJV6Q"
   },
   "outputs": [],
   "source": [
    "#注册钩子函数\n",
    "class saveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZodCLPRUJV6S"
   },
   "outputs": [],
   "source": [
    "class unetUpSampleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    用于创建unet右侧的上采样层，采用转置卷积进行上采样（尺寸×2）\n",
    "    self.tranConv将上一层进行上采样，尺寸×2\n",
    "    self.conv，将左侧特征图再做一次卷积减少通道数，所以尺寸不变\n",
    "    此时两者尺寸正好一致-----建立在图片尺寸为128×128的基础上，否则上采样不能简单的×2\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels,feature_channels,out_channels,dp=False,ps=0.25):#注意，out_channels 是最终输出通道的一半。\n",
    "        super(unetUpSampleBlock,self).__init__()\n",
    "        self.tranConv = nn.ConvTranspose2d(in_channels,out_channels,kernel_size=2,stride=2,bias=False)#输出尺寸正好为输入尺寸的两倍\n",
    "        self.conv = nn.Conv2d(feature_channels,out_channels,1,bias=False) #这一层将传来的特征图再做一次卷积，将特征图通道数减半\n",
    "        self.bn = nn.BatchNorm2d(out_channels*2) #将特征图与上采样再通道出相加后再一起归一化\n",
    "        self.dp = dp\n",
    "        if dp:\n",
    "            self.dropout = nn.Dropout(ps,inplace=True)\n",
    "            \n",
    "    def forward(self,x,features):\n",
    "        x1 = self.tranConv(x)\n",
    "        x2 = self.conv(features)\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.bn(F.relu(x))\n",
    "        return self.dropout(x) if self.dp else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFhhkVPtJV6U"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #基于resnet50的UNet网络\n",
    "    #NIR是可见光模式，3通道\n",
    "    #主干网络为Unet，输入输出尺寸均为64×64\n",
    "    def __init__(self,model,in_channels,out_channels):\n",
    "        super(Generator,self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels,64,kernel_size=3,stride=1,padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(inplace = True)\n",
    "        )\n",
    "        self.downsample = nn.Sequential(*list(model.children())[4:-2])\n",
    "        #print(len(list(model.children())[4:-2]))\n",
    "        self.features = [saveFeatures(list(self.downsample.children())[i]) for i in range(3)]\n",
    "        self.up1 = unetUpSampleBlock(2048,1024,512) #feature:self.features[2]\n",
    "        self.up2 = unetUpSampleBlock(1024,512,256)\n",
    "        self.up3 = unetUpSampleBlock(512,256,128)\n",
    "        self.up4 = unetUpSampleBlock(256,64,32) #feature:self.layer1的输出\n",
    "        self.outlayer = nn.Conv2d(64,out_channels,3,1,1)\n",
    "        \n",
    "    def forward(self,batch):\n",
    "        out_batch = []\n",
    "        for i in batch:\n",
    "            x1 = self.layer1(i)\n",
    "            x = self.downsample(x1)\n",
    "            x = self.up1(x,self.features[2].features)\n",
    "            x = self.up2(x,self.features[1].features)\n",
    "            x = self.up3(x,self.features[0].features)\n",
    "            x = self.up4(x,x1)\n",
    "            out_batch.append(self.outlayer(x))\n",
    "        return out_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2295,
     "status": "ok",
     "timestamp": 1563795189389,
     "user": {
      "displayName": "黄翼",
      "photoUrl": "",
      "userId": "03920361838354357255"
     },
     "user_tz": -480
    },
    "id": "Px0W3WUJJV6V",
    "outputId": "08933975-0f64-4275-bff1-2d57884858c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n",
      "100%|██████████| 102502400/102502400 [00:00<00:00, 158307478.63it/s]\n"
     ]
    }
   ],
   "source": [
    "m = models.resnet50(pretrained=True)\n",
    "tem_paras = copy.deepcopy(m.layer1[0].downsample[0].state_dict())\n",
    "m.layer1[0].downsample[0] = nn.Conv2d(64, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "m.layer1[0].downsample[0].load_state_dict(tem_paras)\n",
    "tem_paras = copy.deepcopy(m.layer1[0].conv2.state_dict())\n",
    "m.layer1[0].conv2 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "m.layer1[0].conv2.load_state_dict(tem_paras)\n",
    "del tem_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3PJtBFCJV6X"
   },
   "outputs": [],
   "source": [
    "genernator_VIS2NIR = Generator(m,3,1)\n",
    "genernator_NIR2VIS = Generator(m,1,3)\n",
    "merge_NIR2NIR = outBlock(5,1)\n",
    "merge_VIS2VIS = outBlock(15,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4728,
     "status": "ok",
     "timestamp": 1563795192178,
     "user": {
      "displayName": "黄翼",
      "photoUrl": "",
      "userId": "03920361838354357255"
     },
     "user_tz": -480
    },
    "id": "b6ju7H6bJV6Z",
    "outputId": "2f55665b-375e-40d4-b1e1-f1b834992a86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/checkpoints/resnet34-333f7ec4.pth\n",
      "100%|██████████| 87306240/87306240 [00:00<00:00, 90780405.83it/s]\n"
     ]
    }
   ],
   "source": [
    "discriminator_A_NIR = models.resnet34(pretrained=True)\n",
    "discriminator_B_VIS = models.resnet34(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HaXFKohXJV6c"
   },
   "outputs": [],
   "source": [
    "discriminator_A_NIR.fc = nn.Linear(512,1,bias = True)\n",
    "discriminator_B_VIS.fc = nn.Linear(512,1,bias = True)\n",
    "#resnet降的倍数太多了，减少一个pool\n",
    "discriminator_B_VIS.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
    "discriminator_A_NIR.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UlVkWwOsNXAu"
   },
   "outputs": [],
   "source": [
    "genernator_NIR2VIS = genernator_NIR2VIS.to(device)\n",
    "genernator_VIS2NIR = genernator_VIS2NIR.to(device)\n",
    "merge_NIR2NIR = merge_NIR2NIR.to(device)\n",
    "merge_VIS2VIS = merge_VIS2VIS.to(device)\n",
    "discriminator_A_NIR = discriminator_A_NIR.to(device)\n",
    "discriminator_B_VIS = discriminator_B_VIS.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "015pqwL_JV6e"
   },
   "source": [
    "### 以下定义数据读取\n",
    "\n",
    "- 分别读取一张图片即上面的头、胸、手、腿  \n",
    "- 全局图片为.resize((64,128))  \n",
    "- 头为.resize((32,16))\n",
    "- 胸部为.resize((64,64))  \n",
    "- 手臂为.resize((64,64))  \n",
    "- 腿部为.resize((64,128))  \n",
    "- 坐标文件存储在images_NIR.yml和images_VIS.yml两个文件上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNme9wg9JV6f"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import yaml\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-AePwzxJV6h"
   },
   "outputs": [],
   "source": [
    "def process(x1,y1,x2,y2):\n",
    "    return x1,y1,x2,y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0PxutGZgJV6j"
   },
   "outputs": [],
   "source": [
    "class CustomDatasets(Dataset):\n",
    "    def __init__(self,img_NIR_all,img_VIS_all,img_train_NIR_list,img_train_VIS_list,img_NIR_dir,img_VIS_dir):\n",
    "        self.img_NIR_all = img_NIR_all\n",
    "        self.img_VIS_all = img_VIS_all\n",
    "        self.img_train_NIR_list = img_train_NIR_list\n",
    "        self.img_train_VIS_list = img_train_VIS_list\n",
    "        self.img_NIR_dir = img_NIR_dir\n",
    "        self.img_VIS_dir = img_VIS_dir\n",
    "        self.NIR_key = list(img_NIR_all.keys())\n",
    "        self.VIS_key = list(img_VIS_all.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_train_NIR_list)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_NIR_info = self.img_NIR_all[self.NIR_key[idx]]\n",
    "        img_VIS_info = self.img_VIS_all[self.VIS_key[idx]]\n",
    "        \n",
    "        batch = {}\n",
    "        \n",
    "        name = self.NIR_key[idx].split('.')\n",
    "        name = name[0][:-2]+'.'+name[1]\n",
    "        batch['img_NIR'] = Image.open(os.path.join(self.img_NIR_dir,name)).convert('L').resize((64,128))\n",
    "        #如果想要打乱NIR图像与VIS图像之间的关系的话只需重新随机选择一个idx即可\n",
    "        name = self.VIS_key[idx].split('.')\n",
    "        name = name[0][:-2]+'.'+name[1]\n",
    "        batch['img_VIS'] = Image.open(os.path.join(self.img_VIS_dir,name)).convert('RGB').resize((64,128))\n",
    "        \n",
    "        batch['id_NIR'] = int(self.NIR_key[idx].split('_')[0])\n",
    "        batch['id_VIS'] = int(self.VIS_key[idx].split('_')[0])\n",
    "        \n",
    "        batch['head_NIR'] = batch['img_NIR'].crop(process(**img_NIR_info['head'])).resize((32,16))\n",
    "        batch['head_VIS'] = batch['img_VIS'].crop(process(**img_NIR_info['head'])).resize((32,16))\n",
    "        \n",
    "        batch['chest_NIR'] = batch['img_NIR'].crop(process(**img_NIR_info['chest'])).resize((64,64))\n",
    "        batch['chest_VIS'] = batch['img_VIS'].crop(process(**img_NIR_info['chest'])).resize((64,64))\n",
    "        \n",
    "        batch['thigh_NIR'] = batch['img_NIR'].crop(process(**img_NIR_info['thigh'])).resize((64,64))\n",
    "        batch['thigh_VIS'] = batch['img_VIS'].crop(process(**img_NIR_info['thigh'])).resize((64,64))\n",
    "        \n",
    "        batch['leg_NIR'] = batch['img_NIR'].crop(process(**img_NIR_info['leg'])).resize((64,128))\n",
    "        batch['leg_VIS'] = batch['img_VIS'].crop(process(**img_NIR_info['leg'])).resize((64,128))\n",
    "        \n",
    "        totensor = transforms.ToTensor()\n",
    "        for i in batch.keys():\n",
    "            if i == 'id_NIR' or i == 'id_VIS':\n",
    "                continue\n",
    "            batch[i] = totensor(batch[i])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ag39T2G6JV6k"
   },
   "outputs": [],
   "source": [
    "def createDatasets(yaml_NIR,yaml_VIS,img_NIR_dir,img_VIS_dir,p_test=0.1):\n",
    "    with open(yaml_NIR,'r') as rf:\n",
    "        img_NIR_all = yaml.safe_load(rf.read())\n",
    "    with open(yaml_VIS,'r') as rf:\n",
    "        img_VIS_all = yaml.safe_load(rf.read())\n",
    "        \n",
    "    #假设img_NIR_all和img_VIS_all长度一致\n",
    "    length = min(len(img_NIR_all),len(img_VIS_all))\n",
    "    \n",
    "    img_test_NIR_list = list(img_NIR_all.keys())[:int(length*p_test)]\n",
    "    img_test_VIS_list = list(img_VIS_all.keys())[:int(length*p_test)]\n",
    "    img_train_NIR_list = list(img_NIR_all.keys())[int(length*p_test):length]\n",
    "    img_train_VIS_list = list(img_VIS_all.keys())[int(length*p_test):length]\n",
    "    #return img_NIR_all,img_VIS_all,img_train_NIR_list,img_train_VIS_list,img_NIR_dir,img_VIS_dir\n",
    "    return CustomDatasets(img_NIR_all,img_VIS_all,img_train_NIR_list,img_train_VIS_list,img_NIR_dir,img_VIS_dir),CustomDatasets(img_NIR_all,img_VIS_all,img_test_NIR_list,img_test_VIS_list,img_NIR_dir,img_VIS_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WcuMi5ThJV6m"
   },
   "outputs": [],
   "source": [
    "trainSet,testSet = createDatasets('./images_NIR.yml','./images_VIS.yml','./data/trainB/','./data/trainA/')\n",
    "train_loader = data.DataLoader(trainSet,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader = data.DataLoader(testSet,1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWQrRV2xJV6o"
   },
   "outputs": [],
   "source": [
    "def concat_patch(b):\n",
    "    size = b[0].size()#遮掩新生成的一个tensor，反向传播会在此停止。\n",
    "    batch = torch.zeros(size[0],size[1]*5,size[2],size[3])\n",
    "    batch = batch.to(device)\n",
    "    batch[:,:size[1],:,:] = b[0]\n",
    "    batch[:,size[1]:size[1]*2,:16,16:48] = b[1]\n",
    "    batch[:,size[1]*2:size[1]*3,:64,:] = b[2]\n",
    "    batch[:,size[1]*3:size[1]*4,:64,:] = b[3]\n",
    "    #print(batch.size(),batch[:,size[1]*4,:,:].size(),b[4].size())\n",
    "    batch[:,size[1]*4:,:,:] = b[4]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L60D-EV5JV6p"
   },
   "outputs": [],
   "source": [
    "def deconcat(batch):#如果上面合并时不生成一个新的tensor，直接操作会改变batch的形状，可以用这个转回来\n",
    "    batch[1] = batch[1][:,:,:16,16:48]\n",
    "    batch[2] = batch[2][:,:,:64,:]\n",
    "    batch[3] = batch[3][:,:,:64,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U_W2x259JV6r"
   },
   "source": [
    "### 以下定义训练及测试\n",
    "\n",
    "- 训练过程：先生成5各VIS图，再把它们合并再生成更清楚的全局VIS图\n",
    "- 得到的全军图的size为[batchsize,channels,128,64]\n",
    "- 得到的headsize为[batchsize,channels,16,32] ---- 所有的图片均作了一个转置\n",
    "- 生成器和判别器的loss均不要取log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yYWNpQ2tJV6s"
   },
   "source": [
    "对于生成器，其前向传播过程为生成器A生成fake1（五张图片），再合并这五张图片然后再生成fake2（一张图片，最终要得到的就是这张），然后fake1的local+fake2传递给生成器B，同理生成fake3和fake4.  \n",
    "同时，fake1和fake2交给判别器打分，我们希望生成的这些图片再判别器中获得的分数越高越好，所以定义损失为||socre-1||，然后反向传播，得到梯度。  \n",
    "同时，fake3，fake4应该与原图越相似越好，所以定义loss: norm(fake-real)，反向传播得到梯度。  \n",
    "两次得到的梯度信息合并然后更新生成器A的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "seiCrPRkJV6s"
   },
   "outputs": [],
   "source": [
    "#如果只是L1番薯，则loss会特别大，可以改用mean(abs(map))\n",
    "def similarity_loss(real,fake):\n",
    "    loss = 0\n",
    "    for i,j in zip(real,fake):\n",
    "        loss += torch.mean(torch.abs(i-j))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xmr5TdjHJV6u"
   },
   "outputs": [],
   "source": [
    "def score_loss(discrinminator,fake):\n",
    "    loss = 0\n",
    "    for i in fake:\n",
    "        loss += torch.pow(discrinminator(i.expand(-1,3,-1,-1))-1,2) \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAxqI2XsJV6x"
   },
   "outputs": [],
   "source": [
    "def genernator_train(genernator,merge,discriminator,optim,data_batch):#两个生成器的模型一起放在列表中传入，如[genernator_VIS,genernator_NIR]\n",
    "    genernator[0].train()\n",
    "    genernator[1].train()\n",
    "    merge[0].train()\n",
    "    merge[1].train()\n",
    "    discriminator[0].eval()\n",
    "    discriminator[1].eval()    \n",
    "    \n",
    "    VIS_real = [data_batch['img_VIS'],data_batch['head_VIS'],data_batch['chest_VIS'],data_batch['thigh_VIS'],data_batch['leg_VIS']]\n",
    "    NIR_real = [data_batch['img_NIR'],data_batch['head_NIR'],data_batch['chest_NIR'],data_batch['thigh_NIR'],data_batch['leg_NIR']]\n",
    "    \n",
    "    VIS2NIR_fake1 = genernator[0](VIS_real)\n",
    "    #print(VIS2NIR_fake1[0].shape,VIS2NIR_fake1[1].shape,VIS2NIR_fake1[2].shape,VIS2NIR_fake1[3].shape,VIS2NIR_fake1[4].shape)\n",
    "    batch_fake = concat_patch(VIS2NIR_fake1)\n",
    "    #print(VIS2NIR_fake1[0].shape,VIS2NIR_fake1[1].shape,VIS2NIR_fake1[2].shape,VIS2NIR_fake1[3].shape,VIS2NIR_fake1[4].shape)\n",
    "    VIS2NIR_fake2 = merge[0](batch_fake)\n",
    "    NIR2VIS_fake3 = genernator[1]([VIS2NIR_fake2,*VIS2NIR_fake1[1:]])\n",
    "    #print(NIR2VIS_fake3[0].shape,NIR2VIS_fake3[1].shape,NIR2VIS_fake3[2].shape,NIR2VIS_fake3[3].shape,NIR2VIS_fake3[4].shape)\n",
    "    batch_fake = concat_patch(NIR2VIS_fake3)\n",
    "    NIR2VIS_fake4 = merge[1](batch_fake)\n",
    "    loss_A_consistency = similarity_loss([NIR2VIS_fake4,*NIR2VIS_fake3],[VIS_real[0],*VIS_real])\n",
    "    loss_A_discriminator = score_loss(discriminator[0],[VIS2NIR_fake2,*VIS2NIR_fake1])\n",
    "    del VIS2NIR_fake1,VIS2NIR_fake2,NIR2VIS_fake3,NIR2VIS_fake4,batch_fake\n",
    "    \n",
    "    NIR2VIS_fake1 = genernator[1](NIR_real)\n",
    "    batch_fake = concat_patch(NIR2VIS_fake1)\n",
    "    NIR2VIS_fake2 = merge[1](batch_fake)\n",
    "    VIS2NIR_fake3 = genernator[0]([NIR2VIS_fake2,*NIR2VIS_fake1[1:]])\n",
    "    batch_fake = concat_patch(VIS2NIR_fake3)\n",
    "    VIS2NIR_fake4 = merge[0](batch_fake)\n",
    "    loss_B_consistency = similarity_loss([VIS2NIR_fake4,*VIS2NIR_fake3],[NIR_real[0],*NIR_real])\n",
    "    loss_B_discriminator = score_loss(discriminator[1],[NIR2VIS_fake2,*NIR2VIS_fake1])\n",
    "    del NIR2VIS_fake1,NIR2VIS_fake2,VIS2NIR_fake3,VIS2NIR_fake4\n",
    "    del VIS_real,NIR_real\n",
    "    \n",
    "    loss_genernator = loss_A_consistency*2+loss_A_discriminator+loss_B_consistency*2+loss_B_discriminator\n",
    "    optim[0].zero_grad()\n",
    "    optim[1].zero_grad()\n",
    "    optim[2].zero_grad()\n",
    "    optim[3].zero_grad()\n",
    "    #print(loss_genernator)\n",
    "    loss_genernator.mean().backward()\n",
    "    optim[0].step()\n",
    "    optim[1].step()\n",
    "    optim[2].step()\n",
    "    optim[3].step()\n",
    "    \n",
    "    return loss_A_consistency,loss_A_discriminator,loss_B_consistency,loss_B_discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGpWcTpGJV6z"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(discriminator,fake,real):\n",
    "    loss = 0\n",
    "    for i,j in zip(fake,real):\n",
    "        #print(j.expand(-1,3,-1,-1).shape)\n",
    "        loss += (torch.pow(discriminator(j.expand(-1,3,-1,-1))-1,2)+torch.pow(discriminator(i.expand(-1,3,-1,-1)),2))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YI8bDWcJV61"
   },
   "outputs": [],
   "source": [
    "def discriminator_train(discriminator,genernator,merge,optim,data_batch):#参数的传递方式同上\n",
    "    discriminator[0].train()\n",
    "    discriminator[1].train()\n",
    "    genernator[0].eval()\n",
    "    genernator[1].eval()\n",
    "    merge[0].eval()\n",
    "    merge[1].eval()\n",
    "    VIS_real = [data_batch['img_VIS'],data_batch['head_VIS'],data_batch['chest_VIS'],data_batch['thigh_VIS'],data_batch['leg_VIS']]\n",
    "    NIR_real = [data_batch['img_NIR'],data_batch['head_NIR'],data_batch['chest_NIR'],data_batch['thigh_NIR'],data_batch['leg_NIR']]\n",
    "    \n",
    "    VIS2NIR_fake1 = genernator[0](VIS_real)\n",
    "    for i in VIS2NIR_fake1:\n",
    "        i.detach()\n",
    "    batch_fake = concat_patch(VIS2NIR_fake1)\n",
    "    VIS2NIR_fake2 = merge[0](batch_fake).detach()\n",
    "    loss = discriminator_loss(discriminator[0],[VIS2NIR_fake2,*VIS2NIR_fake1],[VIS_real[0],*VIS_real])\n",
    "    del VIS2NIR_fake1,VIS2NIR_fake2\n",
    "    optim[0].zero_grad()\n",
    "    loss.mean().backward()\n",
    "    optim[0].step()\n",
    "    \n",
    "    \n",
    "    NIR2VIS_fake1 = genernator[1](NIR_real)\n",
    "    for i in NIR2VIS_fake1:\n",
    "        i.detach()\n",
    "    batch_fake = concat_patch(NIR2VIS_fake1)\n",
    "    NIR2VIS_fake2 = merge[1](batch_fake).detach()\n",
    "    loss_1 = discriminator_loss(discriminator[1],[NIR2VIS_fake2,*NIR2VIS_fake1],[NIR_real[0],*NIR_real])\n",
    "    del NIR2VIS_fake1,NIR2VIS_fake2,batch_fake,VIS_real,NIR_real\n",
    "    optim[1].zero_grad()\n",
    "    loss_1.mean().backward()\n",
    "    optim[1].step()\n",
    "    \n",
    "    return loss,loss_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SuLMdeTvJV62"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsHKTRduJV64"
   },
   "outputs": [],
   "source": [
    "def test(genernator,merge,data_batch,epoch):\n",
    "    genernator[0].eval()\n",
    "    genernator[1].eval()\n",
    "    merge[0].eval()\n",
    "    merge[1].eval()\n",
    "    transform = transforms.ToPILImage()\n",
    "    for i in data_batch.keys():\n",
    "        data_batch[i] = data_batch[i].to(device)\n",
    "    with torch.no_grad():\n",
    "        VIS_real = [data_batch['img_VIS'],data_batch['head_VIS'],data_batch['chest_VIS'],data_batch['thigh_VIS'],data_batch['leg_VIS']]\n",
    "        NIR_real = [data_batch['img_NIR'],data_batch['head_NIR'],data_batch['chest_NIR'],data_batch['thigh_NIR'],data_batch['leg_NIR']]\n",
    "    \n",
    "        VIS2NIR_fake1 = genernator[0](VIS_real)\n",
    "        batch_fake = concat_patch(VIS2NIR_fake1)\n",
    "        VIS2NIR_fake2 = merge[0](batch_fake)\n",
    "    \n",
    "        NIR2VIS_fake3 = genernator[1]([VIS2NIR_fake2,*VIS2NIR_fake1[1:]])\n",
    "        batch_fake = concat_patch(NIR2VIS_fake3)\n",
    "        NIR2VIS_fake4 = merge[1](batch_fake)\n",
    "    \n",
    "    fig=plt.figure(figsize=(16, 4))\n",
    "    columns = 6\n",
    "    rows = 1\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    #print(VIS_real[0].shape)\n",
    "    plt.imshow(transform(VIS_real[0][0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(transform(VIS2NIR_fake1[0][0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(transform(VIS2NIR_fake2[0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 4)\n",
    "    plt.imshow(transform(NIR2VIS_fake3[0][0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 5)\n",
    "    plt.imshow(transform(NIR2VIS_fake4[0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 6)\n",
    "    plt.imshow(transform(NIR_real[0][0].cpu()))\n",
    "    plt.tight_layout()       \n",
    "    plt.savefig('./process_image/image_VIS2NIR_A_%d.jpg'%(epoch+1))\n",
    "    plt.show()\n",
    "    \n",
    "    del VIS2NIR_fake1,VIS2NIR_fake2,NIR2VIS_fake3,NIR2VIS_fake4,batch_fake\n",
    "    with torch.no_grad():\n",
    "        NIR2VIS_fake1 = genernator[1](NIR_real)\n",
    "        batch_fake = concat_patch(NIR2VIS_fake1)\n",
    "        NIR2VIS_fake2 = merge[1](batch_fake)\n",
    "        VIS2NIR_fake3 = genernator[0]([NIR2VIS_fake2,*NIR2VIS_fake1[1:]])\n",
    "        batch_fake = concat_patch(VIS2NIR_fake3)\n",
    "        VIS2NIR_fake4 = merge[0](batch_fake)\n",
    "    \n",
    "    fig=plt.figure(figsize=(16, 4))\n",
    "    columns = 6\n",
    "    rows = 1\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(transform(NIR_real[0][0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(transform(NIR2VIS_fake1[0][0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(transform(NIR2VIS_fake2[0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 4)\n",
    "    plt.imshow(transform(VIS2NIR_fake3[0][0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 5)\n",
    "    plt.imshow(transform(VIS2NIR_fake4[0].cpu()))\n",
    "    fig.add_subplot(rows, columns, 6)\n",
    "    plt.imshow(transform(VIS_real[0][0].cpu()))\n",
    "    plt.tight_layout()       \n",
    "    plt.savefig('./process_image/image_NIR2VIS_B_%d.jpg'%(epoch+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2ziN9K2JV65"
   },
   "source": [
    "### 以下为正式训练的流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGPnGQOoJV66"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHUJVkLXJV67"
   },
   "outputs": [],
   "source": [
    "optimzer_gen_A_VIS2NIR = optim.RMSprop(genernator_VIS2NIR.parameters(),lr=0.0002)\n",
    "optimzer_gen_B_NIR2VIS = optim.RMSprop(genernator_NIR2VIS.parameters(),lr=0.0002)\n",
    "optimzer_mer_A_NIR2NIR = optim.RMSprop(merge_NIR2NIR.parameters(),lr=0.0002)\n",
    "optimzer_mer_B_VIS2VIS = optim.RMSprop(merge_VIS2VIS.parameters(),lr=0.0002)\n",
    "optimzer_dis_A = optim.Adam(discriminator_A_NIR.parameters(),lr = 0.001)\n",
    "optimzer_dis_B = optim.Adam(discriminator_B_VIS.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1_YaLAHX9nY2TTwt9guNSH7BCtPteXCkY"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42889169,
     "status": "error",
     "timestamp": 1563838366513,
     "user": {
      "displayName": "黄翼",
      "photoUrl": "",
      "userId": "03920361838354357255"
     },
     "user_tz": -480
    },
    "id": "wc7mx8tWJV69",
    "outputId": "c13d853a-e1fb-4b1c-d286-45e52ef4cdca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    test([genernator_VIS2NIR,genernator_NIR2VIS],[merge_NIR2NIR,merge_VIS2VIS],next(iter(test_loader)),epoch)\n",
    "    for batch in train_loader:\n",
    "        for i in batch.keys():\n",
    "            batch[i] = batch[i].to(device)\n",
    "        loss_A_consistency,loss_A_discriminator,loss_B_consistency,loss_B_discriminator = genernator_train([genernator_VIS2NIR,genernator_NIR2VIS],[merge_NIR2NIR,merge_VIS2VIS],[discriminator_A_NIR,discriminator_B_VIS],[optimzer_gen_A_VIS2NIR,optimzer_gen_B_NIR2VIS,optimzer_mer_A_NIR2NIR,optimzer_mer_B_VIS2VIS],batch)\n",
    "        loss,loss_1 = discriminator_train([discriminator_A_NIR,discriminator_B_VIS],[genernator_VIS2NIR,genernator_NIR2VIS],[merge_NIR2NIR,merge_VIS2VIS],[optimzer_dis_A,optimzer_dis_B],batch)\n",
    "    print('epoch: {}/{},loss_A_consistency: {},loss_A_discriminator: {},loss_B_consistency: {},loss_B_discriminator: {}'.format(epoch+1,EPOCHS,loss_A_consistency,loss_A_discriminator.item(),loss_B_consistency,loss_B_discriminator.item()))\n",
    "    print('discriminator_A_VIS_loss:{},discriminator_B_NIR_loss{}'.format(loss.item(),loss_1.item()))\n",
    "    if epoch%10 == 0:\n",
    "        torch.save(genernator_VIS2NIR.state_dict(), './process_image/genernator_VIS2NIR.pkl')\n",
    "        torch.save(genernator_NIR2VIS.state_dict(), './process_image/genernator_NIR2VIS.pkl')\n",
    "        torch.save(merge_NIR2NIR.state_dict(), './process_image/merge_NIR2NIR.pkl')\n",
    "        torch.save(merge_VIS2VIS.state_dict(), './process_image/merge_VIS2VIS.pkl')\n",
    "        torch.save(discriminator_A_NIR.state_dict(), './process_image/discriminator_A_NIR.pkl')\n",
    "        torch.save(discriminator_B_VIS.state_dict(), './process_image/discriminator_B_VIS.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hocCj_8_JV7A"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FECXKsM1JV7C"
   },
   "outputs": [],
   "source": [
    "test([genernator_VIS2NIR,genernator_NIR2VIS],[merge_NIR2NIR,merge_VIS2VIS],next(iter(test_loader)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "new.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
